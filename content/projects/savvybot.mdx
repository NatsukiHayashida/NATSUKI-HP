---
title: "Savybot バージョンアップログ"
date: "2025-01-10"
excerpt: "Claude / GPT を活用したチャットアプリ「Savybot」の進化記録。v1.0からv3.0への改善プロセスと成果を公開。"
category: "AI / Webアプリ開発"
tags: ["AI", "ChatBot", "Claude", "GPT-4", "TypeScript", "Vercel"]
demoUrl: "https://savybot.vercel.app"
technologies: ["Next.js 14", "TypeScript", "OpenAI API", "Claude API", "Vercel AI SDK", "Tailwind CSS"]
role: "Full Stack Developer"
duration: "2024年6月〜現在（進行中）"
outcomes:
  - "応答時間75%短縮（7.2s → 1.8s）"
  - "月間アクティブユーザー400%増（50人 → 250人）"
  - "API利用コスト60%削減（$45 → $18/月）"
  - "ユーザー満足度4.6/5.0達成"
challenges:
  - "複数AIモデルの統合と最適な使い分け"
  - "ストリーミング応答の実装とUI最適化"
  - "コスト効率化とレスポンス品質のバランス"
learnings:
  - "AIモデルの特性を活かしたハイブリッド設計"
  - "ストリーミングUIによるUX改善手法"
  - "プロンプトエンジニアリングとコスト最適化"
---

## プロジェクト背景

2024年6月、個人的な実験として「Savybot」というAIチャットアプリを開発しました。当初はOpenAI GPT-3.5を使ったシンプルなチャットボットでしたが、ユーザーフィードバックと技術進化に合わせて継続的に改善してきました。

### なぜ改善を続けるのか

1. **技術的好奇心**
   - 新しいAIモデル（Claude 3.5、GPT-4o）の実践的な活用
   - ストリーミング応答やマルチモーダルAIの実装経験

2. **ユーザーニーズの変化**
   - より高速で自然な対話体験への期待
   - 用途に応じた柔軟なAIモデル選択

3. **コスト最適化の必要性**
   - 個人プロジェクトとしての持続可能性
   - ユーザー増加に伴うAPI コストの管理

---

## バージョン進化ログ

### v1.0（2024年6月）：シンプルなチャットボット

**実装内容：**
- OpenAI GPT-3.5 Turbo統合
- 基本的なチャットUI
- 会話履歴の保存（セッションのみ）

**技術スタック：**
- Next.js 14
- OpenAI API
- React

**制限事項：**
- 応答速度が遅い（平均7.2秒）
- コンテキストがセッションでリセット
- 単一AIモデルのみ

**KPI：**
- 月間ユーザー：50人
- 応答時間：7.2秒
- API コスト：$15/月

---

### v2.0（2024年9月）：マルチAI対応とストリーミング

**新機能：**
1. **マルチAIモデル統合**
   - Claude 3.5 Sonnet追加
   - GPT-4 Turbo追加
   - ユーザーがモデルを選択可能に

2. **ストリーミング応答**
   - Vercel AI SDKでストリーミング実装
   - リアルタイム応答表示
   - 体感速度の大幅改善

3. **UI改善**
   - モバイル最適化
   - Markdown/コードハイライト対応
   - ダークモード対応

**実装例：**
```typescript
// app/api/chat/route.ts
import { StreamingTextResponse, OpenAIStream } from 'ai'
import OpenAI from 'openai'

const openai = new OpenAI()

export async function POST(req: Request) {
  const { messages, model } = await req.json()

  const response = await openai.chat.completions.create({
    model: model === 'gpt4' ? 'gpt-4-turbo' : 'gpt-3.5-turbo',
    messages,
    stream: true,
  })

  const stream = OpenAIStream(response)
  return new StreamingTextResponse(stream)
}
```

**成果：**
- 応答時間：3.5秒（51%改善）
- 月間ユーザー：120人（140%増）
- API コスト：$35/月（ユーザー増に対応）

**新たな課題：**
- 複数モデルで設定が分散
- コストが増加傾向
- モバイルUIがまだ改善の余地あり

---

### v3.0（2024年12月〜現在）：コスト最適化と高度な機能

**改善ポイント：**

#### 1. ストリーミング応答の最適化

リアルタイム応答表示でUX向上：

```typescript
// app/api/chat/route.ts
import { streamText } from 'ai'
import { openai } from '@ai-sdk/openai'
import { anthropic } from '@ai-sdk/anthropic'

export async function POST(req: Request) {
  const { messages, provider } = await req.json()

  const model = provider === 'claude'
    ? anthropic('claude-3-5-sonnet-20241022')
    : openai('gpt-4-turbo')

  const result = await streamText({
    model,
    messages,
    temperature: 0.7,
  })

  return result.toDataStreamResponse()
}
```

**UIコンポーネント：**
```tsx
// components/chat.tsx
'use client'
import { useChat } from 'ai/react'

export function Chat() {
  const { messages, input, handleSubmit, isLoading } = useChat()

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          <ReactMarkdown>{m.content}</ReactMarkdown>
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input value={input} />
        <button disabled={isLoading}>送信</button>
      </form>
    </div>
  )
}
```

#### 2. コスト効率化

**戦略：**
- 簡単な質問はGPT-3.5-turbo（安価）
- 複雑な質問はGPT-4/Claude（高品質）
- システムプロンプト最適化でトークン削減

```typescript
// lib/model-selector.ts
export function selectModel(message: string): AIModel {
  const complexity = analyzeComplexity(message)

  if (complexity === 'low') {
    return 'gpt-3.5-turbo' // コスト: $0.001/1K tokens
  }

  if (complexity === 'high' && needsReasoning(message)) {
    return 'claude-3-5-sonnet' // 高度な推論
  }

  return 'gpt-4-turbo' // バランス型
}

function analyzeComplexity(message: string): 'low' | 'medium' | 'high' {
  const wordCount = message.split(' ').length
  const hasCodeRequest = /code|program|script/.test(message)
  const hasAnalysisRequest = /analyze|compare|evaluate/.test(message)

  if (wordCount < 10 && !hasCodeRequest && !hasAnalysisRequest) {
    return 'low'
  }

  if (hasCodeRequest || hasAnalysisRequest) {
    return 'high'
  }

  return 'medium'
}
```

#### 3. UI改善とモバイル対応

**改善内容：**
- レスポンシブデザイン刷新
- プリセットプロンプト機能
- コードブロックのシンタックスハイライト
- 会話エクスポート機能

**モバイル最適化：**
```tsx
// components/mobile-chat.tsx
export function MobileChat() {
  return (
    <div className="flex flex-col h-screen">
      {/* 固定ヘッダー */}
      <header className="sticky top-0 z-10 bg-background/95 backdrop-blur">
        <ModelSelector />
      </header>

      {/* スクロール可能なメッセージエリア */}
      <div className="flex-1 overflow-y-auto pb-24">
        <Messages />
      </div>

      {/* 固定入力フォーム */}
      <div className="fixed bottom-0 w-full p-4 bg-background/95 backdrop-blur">
        <ChatInput />
      </div>
    </div>
  )
}
```

---

## 成果・KPI

### パフォーマンス改善

| 指標 | v1.0 | v2.0 | v3.0 | 改善率 |
|------|------|------|------|--------|
| 平均応答時間 | 7.2秒 | 3.5秒 | 1.8秒 | **75%改善** |
| 初回トークン表示 | - | 1.2秒 | 0.4秒 | **67%改善** |
| モバイルLCP | 3.8秒 | 2.1秒 | 1.1秒 | **71%改善** |

### ユーザー成長

| 指標 | v1.0 | v2.0 | v3.0 | 成長率 |
|------|------|------|------|--------|
| 月間アクティブユーザー | 50人 | 120人 | 250人 | **400%増** |
| 1日あたり会話数 | 150 | 420 | 980 | **553%増** |
| ユーザー満足度 | 3.8/5 | 4.2/5 | 4.6/5 | **21%向上** |

### コスト最適化

| 指標 | v1.0 | v2.0 | v3.0 | 削減率 |
|------|------|------|------|--------|
| 月間API コスト | $15 | $35 | $18 | **60%削減**（v2比） |
| ユーザーあたりコスト | $0.30 | $0.29 | $0.072 | **76%削減** |
| 平均トークン使用量 | 1,200 | 1,100 | 650 | **41%削減** |

---

## 技術的な学び

### 1. AIモデルの使い分け

**学んだこと：**
- GPT-3.5: 簡単な質問、速度重視の場合に最適
- GPT-4: バランス型、幅広い用途に対応
- Claude 3.5: 長文生成、複雑な推論に強い

**実践的な使い分けルール：**
```typescript
const MODEL_SELECTION_RULES = {
  // 簡単な質問 → GPT-3.5（速度・コスト優先）
  simple: {
    model: 'gpt-3.5-turbo',
    triggers: ['天気', '時間', '計算', '翻訳'],
    maxTokens: 500,
  },

  // コード関連 → GPT-4（高精度）
  coding: {
    model: 'gpt-4-turbo',
    triggers: ['code', 'debug', 'プログラム'],
    maxTokens: 2000,
  },

  // 長文・分析 → Claude（推論力）
  analysis: {
    model: 'claude-3-5-sonnet',
    triggers: ['分析', '比較', '要約', 'レビュー'],
    maxTokens: 4000,
  },
}
```

### 2. ストリーミングUIのベストプラクティス

**重要ポイント：**
- 初回トークン表示を最優先（0.5秒以内）
- 途中でローディング表示しない（体感速度低下）
- Markdownレンダリングは完了後に適用

### 3. プロンプトエンジニアリング

**トークン削減テクニック：**
- システムプロンプトを簡潔に（500→150トークン）
- 会話履歴は最新3ターンのみ保持
- 重要な情報のみをコンテキストに含める

---

## 次のステップ

### 短期（〜2025年3月）
- [ ] マルチモーダル対応（画像理解）
- [ ] 音声入力機能
- [ ] 会話分岐機能（複数会話の並行管理）

### 中期（〜2025年6月）
- [ ] RAG（検索拡張生成）統合
- [ ] カスタムGPTs対応
- [ ] チーム機能（複数ユーザーで共有）

### 長期（〜2025年12月）
- [ ] ファインチューニングモデルの導入
- [ ] エンタープライズ向け機能
- [ ] 月間ユーザー1,000人達成

---

## 参考リソース

- [Vercel AI SDK Documentation](https://sdk.vercel.ai/docs)
- [OpenAI Platform](https://platform.openai.com/)
- [Anthropic Claude](https://www.anthropic.com/claude)
- [Next.js Streaming Guide](https://nextjs.org/docs/app/building-your-application/routing/loading-ui-and-streaming)
