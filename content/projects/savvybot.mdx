---
title: "SavvyBot進化プロジェクト - LINEチャットボットを「すごいもの」へ"
date: "2025-01-10"
excerpt: "初期のシンプルなLINEチャットボットから、AI駆動の高度な対話システムへ。ユーザー体験を革新する進化の記録。"
category: "AI & Chatbot"
tags: ["LINE", "Chatbot", "AI", "GPT-4", "Node.js", "Evolution"]
demoUrl: "https://line.me/R/ti/p/@savvybot"
technologies: ["LINE Messaging API", "Node.js", "OpenAI GPT-4", "TypeScript", "Firebase", "Vercel"]
role: "Full Stack Developer"
duration: "進行中（2024/06〜）"
outcomes:
  - "初期バージョン完成（2024/06）：基本的な応答機能"
  - "AI統合完了（2024/09）：OpenAI GPT-3.5 Turbo導入"
challenges:
  - "LINEの制約の中での高度な機能実装"
  - "応答速度とAI品質のバランス"
  - "コンテキスト保持とユーザー体験の最適化"
learnings:
  - "LINEプラットフォームの特性を活かした設計"
  - "AI応答の最適化とコスト管理"
  - "ユーザーフィードバックに基づく継続的改善"
---

## プロジェクト背景

2024年6月、シンプルなLINE応答botとしてSavvyBotを開発しました。当時はLINE上でのチャットボット運用が目新しく、基本的なQ&A機能で十分でした。

### なぜ進化が必要なのか

1. **市場の変化**
   - LINEチャットボットは一般化し、基本機能だけでは差別化できない
   - ユーザーはより自然で賢い対話を期待するようになった

2. **技術の進歩**
   - GPT-4など高度なLLMが利用可能に
   - リアルタイム処理とストリーミング応答が実現可能に

3. **ユーザーニーズ**
   - 単純なFAQ応答から、コンテキストを理解した対話へ
   - パーソナライズされた体験の需要

### 進化の方向性

**「すごいもの」の定義**：
- 🧠 **自然な対話**: ユーザーの意図を理解し、文脈に沿った応答
- ⚡ **高速レスポンス**: 3秒以内の応答時間
- 🎯 **パーソナライゼーション**: ユーザーの好みや履歴を記憶
- 🔄 **継続学習**: ユーザーフィードバックから改善
- 🎨 **リッチコンテンツ**: テキストだけでなく、画像・カルーセル・Flex Messageの活用

---

## 技術アーキテクチャの進化

### Version 1.0（2024/06）：基本実装

```
LINE Webhook
  ↓
Node.js Server (Express)
  ↓
Simple Rule-Based Response
  ↓
LINE Reply API
```

**制限事項**：
- 固定された応答パターンのみ
- コンテキスト保持なし
- 複雑な質問に対応不可

### Version 2.0（2024/09）：AI統合

```
LINE Webhook
  ↓
Node.js Server (TypeScript)
  ↓
OpenAI GPT-3.5 Turbo
  ↓
Response Cache (Redis)
  ↓
LINE Reply API
```

**改善点**：
- 自然言語理解による柔軟な応答
- 基本的なコンテキスト保持
- 応答品質の大幅向上

### Version 3.0（2025/01〜）：次世代システム 🚀 進行中

```
LINE Webhook
  ↓
Serverless Functions (Vercel)
  ↓
┌─────────────────────────────┐
│ Orchestration Layer         │
│  - Intent Recognition       │
│  - Context Management       │
│  - Response Strategy        │
└─────────────────────────────┘
  ↓
┌─────────────────────────────┐
│ AI Engine                   │
│  - OpenAI GPT-4             │
│  - Custom Fine-tuned Model  │
│  - Streaming Response       │
└─────────────────────────────┘
  ↓
┌─────────────────────────────┐
│ Data Layer (Firebase)       │
│  - User Profiles            │
│  - Conversation History     │
│  - Learning Data            │
└─────────────────────────────┘
  ↓
LINE Messaging API
  ↓
Rich Content Response
```

**目標機能**：
- マルチターン対話の完全なコンテキスト保持
- ユーザープロファイルに基づくパーソナライズ
- ストリーミング応答による体感速度向上
- リッチコンテンツの動的生成

---

## 実装フェーズ

### Phase 1: 基盤強化（完了 ✅）

- [x] TypeScript移行（型安全性向上）
- [x] Vercel Serverless移行（スケーラビリティ）
- [x] Firebase統合（データ永続化）
- [x] OpenAI GPT-3.5 Turbo統合

### Phase 2: AI高度化（進行中 🔄）

- [x] GPT-4への移行
- [ ] プロンプトエンジニアリング最適化
  - [ ] システムプロンプトのA/Bテスト
  - [ ] Few-shot learningパターンの確立
- [ ] コンテキストウィンドウ管理
  - [ ] 会話履歴の要約アルゴリズム
  - [ ] 重要情報の抽出と保持
- [ ] ストリーミング応答実装
  - [ ] LINE APIの制約内でのストリーミング対応

### Phase 3: パーソナライゼーション（未着手）

- [ ] ユーザープロファイル管理
  - [ ] 好み・興味の学習
  - [ ] 過去の会話からのインサイト抽出
- [ ] レコメンデーション機能
- [ ] 個別化された応答スタイル

### Phase 4: リッチコンテンツ（未着手）

- [ ] Flex Message動的生成
- [ ] 画像生成統合（DALL-E 3）
- [ ] カルーセル・ボタンテンプレート活用
- [ ] Quick Replyの賢い活用

### Phase 5: 学習・最適化（未着手）

- [ ] ユーザーフィードバック収集機能
- [ ] A/Bテストフレームワーク
- [ ] 応答品質の自動評価
- [ ] コスト最適化（モデル選択・キャッシング）

---

## 開発ログ

### 2024-06-15: Version 1.0 リリース

シンプルなLINE応答botとして初期バージョンをリリース。

**実装内容**：
```javascript
// app.js
app.post('/webhook', (req, res) => {
  const events = req.body.events
  events.forEach(async (event) => {
    if (event.type === 'message' && event.message.type === 'text') {
      const userMessage = event.message.text
      const reply = getResponse(userMessage) // ルールベース

      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: reply
      })
    }
  })
  res.status(200).end()
})
```

**課題**：
- 柔軟性がなく、想定外の質問に対応できない
- ユーザー体験が機械的

### 2024-09-10: Version 2.0 - AI統合

OpenAI GPT-3.5 Turboを統合し、自然な対話を実現。

**実装内容**：
```typescript
// ai-handler.ts
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
})

export async function generateResponse(
  userMessage: string,
  conversationHistory: Message[]
): Promise<string> {
  const messages = [
    {
      role: 'system',
      content: 'あなたは親しみやすく、役立つアシスタントです。'
    },
    ...conversationHistory,
    {
      role: 'user',
      content: userMessage
    }
  ]

  const response = await openai.chat.completions.create({
    model: 'gpt-3.5-turbo',
    messages,
    temperature: 0.7,
    max_tokens: 500
  })

  return response.choices[0].message.content
}
```

**成果**：
- 応答品質が劇的に向上
- ユーザー満足度が上昇（体感）

**新たな課題**：
- 応答速度が遅い（平均5-7秒）
- コンテキストが会話セッションでリセットされる
- コストが予想以上

### 2025-01-10: Version 3.0 開発開始

「すごいもの」を目指し、次世代アーキテクチャの構築を開始。

**GPT-4移行**：
```typescript
// ai-engine.ts
export async function generateAdvancedResponse(
  userMessage: string,
  userId: string
): Promise<AIResponse> {
  // ユーザープロファイル取得
  const userProfile = await getUserProfile(userId)

  // 会話履歴取得（最新10件 + 要約）
  const history = await getConversationHistory(userId, 10)
  const summarizedHistory = await summarizeHistory(history)

  // システムプロンプト動的生成
  const systemPrompt = buildSystemPrompt(userProfile)

  const response = await openai.chat.completions.create({
    model: 'gpt-4',
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'assistant', content: summarizedHistory },
      ...history.slice(-3), // 直近3件はそのまま
      { role: 'user', content: userMessage }
    ],
    temperature: 0.8,
    max_tokens: 800
  })

  return {
    text: response.choices[0].message.content,
    tokens: response.usage.total_tokens,
    model: 'gpt-4'
  }
}
```

**プロンプトエンジニアリング**：
```typescript
function buildSystemPrompt(userProfile: UserProfile): string {
  return `
あなたはSavvyBotという名前の親しみやすいAIアシスタントです。

ユーザー情報：
- 名前: ${userProfile.name || '未設定'}
- 興味: ${userProfile.interests.join(', ')}
- 好み: ${userProfile.preferences}

応答ガイドライン：
1. 親しみやすく、でも礼儀正しい口調
2. ユーザーの興味に関連する情報があれば、さりげなく言及
3. 簡潔に、でも有益な情報を提供
4. 絵文字を適度に使用して親しみやすさを演出
5. 長い応答は避け、必要に応じて「もっと詳しく知りたいですか？」と聞く
`.trim()
}
```

---

## 技術的チャレンジ

### 1. LINEのタイムアウト制約

**問題**：LINEは応答タイムアウトが厳しく、GPT-4の応答が間に合わないことがある

**解決策**：
```typescript
// webhook-handler.ts
export async function handleMessage(event: MessageEvent) {
  // 即座に200 OKを返す
  res.status(200).end()

  // バックグラウンドで処理
  processMessageAsync(event).catch(console.error)
}

async function processMessageAsync(event: MessageEvent) {
  // ローディング表示（Typing indicator）
  await client.pushMessage(event.source.userId, {
    type: 'text',
    text: '考え中... 🤔'
  })

  // AI応答生成
  const response = await generateAdvancedResponse(
    event.message.text,
    event.source.userId
  )

  // 結果を送信
  await client.pushMessage(event.source.userId, {
    type: 'text',
    text: response.text
  })
}
```

### 2. コンテキスト管理

**問題**：会話履歴が長くなるとトークンコストが増大

**解決策**：
```typescript
// context-manager.ts
async function summarizeHistory(
  history: Message[]
): Promise<string> {
  if (history.length < 5) return ''

  const oldMessages = history.slice(0, -3) // 直近3件以外
  const conversation = oldMessages
    .map(m => `${m.role}: ${m.content}`)
    .join('\n')

  const summary = await openai.chat.completions.create({
    model: 'gpt-3.5-turbo', // 要約は安価なモデル
    messages: [
      {
        role: 'system',
        content: '以下の会話を簡潔に要約してください。'
      },
      {
        role: 'user',
        content: conversation
      }
    ],
    max_tokens: 150
  })

  return `[過去の会話要約]: ${summary.choices[0].message.content}`
}
```

### 3. コスト最適化

**戦略**：
- よくある質問は事前キャッシュ
- 要約にはGPT-3.5-turbo使用
- 重要度の低い応答はルールベースにフォールバック

```typescript
// cost-optimizer.ts
async function optimizedGenerate(
  userMessage: string,
  userId: string
): Promise<string> {
  // キャッシュチェック
  const cached = await checkCache(userMessage)
  if (cached) return cached

  // 意図分類
  const intent = await classifyIntent(userMessage)

  // 簡単な質問はルールベース
  if (intent.complexity === 'low') {
    return handleSimpleQuery(userMessage, intent)
  }

  // 複雑な質問のみGPT-4使用
  return generateAdvancedResponse(userMessage, userId)
}
```

---

## 成果指標（KPI）

### 現在の状況（Version 2.0）
- **平均応答時間**: 5.2秒
- **ユーザー満足度**: 未計測
- **月間アクティブユーザー**: 約50名
- **月間API コスト**: $15-20

### 目標（Version 3.0）
- **平均応答時間**: <3秒 ⚡
- **ユーザー満足度**: >4.5/5.0 ⭐
- **月間アクティブユーザー**: 200名+
- **月間API コスト**: $30以下（コスパ向上）
- **コンテキスト保持率**: >90%（5ターン以上の会話）

---

## 次のステップ

### 直近（〜2025/02）
- [ ] GPT-4プロンプト最適化完了
- [ ] ストリーミング応答のプロトタイプ
- [ ] ユーザープロファイル基本機能実装

### 中期（〜2025/04）
- [ ] リッチコンテンツ生成機能
- [ ] A/Bテスト基盤構築
- [ ] パフォーマンス目標達成（応答時間 <3秒）

### 長期（〜2025/06）
- [ ] ファインチューニングモデルの導入
- [ ] マルチモーダル対応（画像理解）
- [ ] ユーザー数200名達成

---

## 参考リソース

- [LINE Messaging API Documentation](https://developers.line.biz/ja/docs/messaging-api/)
- [OpenAI GPT-4 Best Practices](https://platform.openai.com/docs/guides/gpt-best-practices)
- [Vercel Serverless Functions](https://vercel.com/docs/functions/serverless-functions)
- [Firebase Realtime Database](https://firebase.google.com/docs/database)
